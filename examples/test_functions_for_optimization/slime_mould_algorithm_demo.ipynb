{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Suppress ALL output for cleaner notebook\nimport warnings\nimport logging\nimport os\nimport sys\nfrom contextlib import redirect_stdout, redirect_stderr\nfrom io import StringIO\n\n# Comprehensive warning and output suppression\nwarnings.filterwarnings('ignore')\nos.environ['PYTHONWARNINGS'] = 'ignore'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# Set root logger to CRITICAL to suppress all INFO messages\nlogging.getLogger().setLevel(logging.CRITICAL)\nlogging.basicConfig(level=logging.CRITICAL)\n\n# Suppress stdout/stderr for imports\nold_stdout = sys.stdout\nold_stderr = sys.stderr\nsys.stdout = StringIO()\nsys.stderr = StringIO()\n\ntry:\n    # Pre-import mealpy to suppress its initialization messages\n    import mealpy\n    from mealpy.bio_based import SMA\n    # Disable mealpy logging after import\n    mealpy_logger = logging.getLogger('mealpy')\n    mealpy_logger.setLevel(logging.CRITICAL)\n    mealpy_logger.disabled = True\nexcept:\n    pass\n\n# Restore stdout/stderr\nsys.stdout = old_stdout\nsys.stderr = old_stderr\n\nprint(\"üîá Output suppression activated for clean notebook execution\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.colors import LogNorm\nimport seaborn as sns\nimport warnings\nimport logging\n\n# Additional suppression for mealpy logging\ndef suppress_mealpy_logging():\n    \"\"\"Comprehensively suppress mealpy logging output\"\"\"\n    # Disable all mealpy loggers\n    for name in logging.root.manager.loggerDict:\n        if 'mealpy' in name:\n            logger = logging.getLogger(name)\n            logger.setLevel(logging.CRITICAL)\n            logger.disabled = True\n    \n    # Also disable the root logger during optimization\n    root_logger = logging.getLogger()\n    root_logger.setLevel(logging.CRITICAL)\n\n# CoFI imports\nfrom cofi import BaseProblem, InversionOptions, Inversion\n\n# Set style for beautiful plots\nplt.style.use('default')\nsns.set_palette(\"husl\")\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\n\nprint(\"üß¨ Welcome to the Slime Mould Algorithm Demo!\")\nprint(\"üìä Imports successful - ready to explore bio-inspired optimization\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Test Function: Modified Himmelblau Function\n",
    "\n",
    "We'll use a modified version of the classic Himmelblau function, which is a well-known multi-modal optimization benchmark. The original Himmelblau function has four global minima, making it an excellent test case for global optimization algorithms.\n",
    "\n",
    "### Function Definition\n",
    "\n",
    "$$f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2 + (x - 3)^2 + (y - 2)^2$$\n",
    "\n",
    "The modification adds a regularization term $(x - 3)^2 + (y - 2)^2$ that creates a preferred global minimum at $(3, 2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_himmelblau(x):\n",
    "    \"\"\"\n",
    "    Modified Himmelblau function with additional regularization term.\n",
    "    \n",
    "    Original function has 4 global minima. The modification adds a bias\n",
    "    towards the point (3, 2), making it the preferred global minimum.\n",
    "    \n",
    "    Args:\n",
    "        x: array-like, [x1, x2] coordinates\n",
    "        \n",
    "    Returns:\n",
    "        float: Function value\n",
    "    \"\"\"\n",
    "    x1, x2 = x[0], x[1]\n",
    "    # Original Himmelblau terms\n",
    "    term1 = (x1**2 + x2 - 11)**2\n",
    "    term2 = (x1 + x2**2 - 7)**2\n",
    "    # Regularization term (bias towards (3, 2))\n",
    "    term3 = (x1 - 3)**2 + (x2 - 2)**2\n",
    "    \n",
    "    return term1 + term2 + term3\n",
    "\n",
    "# Test the function\n",
    "test_point = [3.0, 2.0]\n",
    "print(f\"üìç Function value at (3, 2): {modified_himmelblau(test_point):.6f}\")\n",
    "print(f\"üéØ This should be close to the global minimum!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Visualizing the Optimization Landscape\n",
    "\n",
    "Let's visualize the modified Himmelblau function to understand the optimization challenge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a meshgrid for visualization\n",
    "x_range = np.linspace(-6, 6, 100)\n",
    "y_range = np.linspace(-6, 6, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "# Evaluate function over the grid\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = modified_himmelblau([X[i, j], Y[i, j]])\n",
    "\n",
    "# Create subplots\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "\n",
    "# 3D surface plot\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, linewidth=0)\n",
    "ax1.set_xlabel('x‚ÇÅ')\n",
    "ax1.set_ylabel('x‚ÇÇ')\n",
    "ax1.set_zlabel('f(x‚ÇÅ, x‚ÇÇ)')\n",
    "ax1.set_title('üèîÔ∏è 3D Surface Plot\\nModified Himmelblau Function')\n",
    "ax1.view_init(elev=30, azim=45)\n",
    "fig.colorbar(surf, ax=ax1, shrink=0.5)\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(132)\n",
    "levels = np.logspace(0, 3, 20)  # Logarithmic levels for better visualization\n",
    "contour = ax2.contour(X, Y, Z, levels=levels, cmap='viridis')\n",
    "ax2.contourf(X, Y, Z, levels=levels, cmap='viridis', alpha=0.6)\n",
    "ax2.set_xlabel('x‚ÇÅ')\n",
    "ax2.set_ylabel('x‚ÇÇ')\n",
    "ax2.set_title('üó∫Ô∏è Contour Plot\\nOptimization Landscape')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark the global minimum\n",
    "ax2.plot(3, 2, 'r*', markersize=15, label='Global Minimum (3, 2)')\n",
    "ax2.legend()\n",
    "\n",
    "# Log-scale contour for better detail\n",
    "ax3 = fig.add_subplot(133)\n",
    "contour_log = ax3.contourf(X, Y, Z, levels=levels, norm=LogNorm(), cmap='plasma')\n",
    "ax3.set_xlabel('x‚ÇÅ')\n",
    "ax3.set_ylabel('x‚ÇÇ')\n",
    "ax3.set_title('üî• Log-scale Contour\\nBetter Detail Visualization')\n",
    "ax3.plot(3, 2, 'w*', markersize=15, label='Global Minimum')\n",
    "ax3.legend()\n",
    "fig.colorbar(contour_log, ax=ax3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üé® Visualization complete!\")\n",
    "print(\"üìä The function has multiple local minima with a preferred global minimum at (3, 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß¨ Setting Up the CoFI Optimization Problem\n",
    "\n",
    "Now let's set up our optimization problem using CoFI's elegant framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimization problem using CoFI\n",
    "problem = BaseProblem()\n",
    "problem.set_objective(modified_himmelblau)  # Our objective function\n",
    "problem.set_model_shape((2,))               # 2D optimization problem\n",
    "problem.set_bounds((-6, 6))                 # Search space bounds\n",
    "\n",
    "print(\"‚úÖ CoFI Problem Setup Complete!\")\n",
    "print(f\"üìè Problem dimension: {np.prod(problem.model_shape)}\")\n",
    "print(f\"üéØ Objective function: Modified Himmelblau\")\n",
    "print(f\"üî≤ Search bounds: {problem.bounds}\")\n",
    "print(\"\\nüöÄ Ready for optimization with Slime Mould Algorithm!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Configure SMA optimization options\nsma_options = InversionOptions()\nsma_options.set_tool(\"mealpy.sma\")\nsma_options.set_params(\n    algorithm=\"OriginalSMA\",  # Use the original SMA variant\n    epoch=100,                # Number of iterations\n    pop_size=50,             # Population size (number of slime moulds)\n    pr=0.03,                 # Probability parameter\n    seed=42,                 # For reproducible results\n    verbose=False            # Reduce output for cleaner notebook\n)\n\nprint(\"‚öôÔ∏è SMA Configuration:\")\nprint(f\"   üîß Algorithm: OriginalSMA\")\nprint(f\"   üîÑ Epochs: 100\")\nprint(f\"   üë• Population: 50 slime moulds\")\nprint(f\"   üé≤ Seed: 42 (reproducible results)\")\n\n# Run the optimization with comprehensive output suppression\nprint(\"\\nüöÄ Starting SMA optimization...\")\n\n# Comprehensive suppression: both stdout and logging\nimport sys\nfrom io import StringIO\n\n# Store original logging level\noriginal_log_level = logging.getLogger().level\n\n# Suppress both stdout and logging\nold_stdout = sys.stdout\nold_stderr = sys.stderr\nsys.stdout = StringIO()\nsys.stderr = StringIO()\n\ntry:\n    # Suppress mealpy logging\n    suppress_mealpy_logging()\n    \n    sma_inversion = Inversion(problem, sma_options)\n    sma_result = sma_inversion.run()\nfinally:\n    # Restore stdout/stderr and logging\n    sys.stdout = old_stdout\n    sys.stderr = old_stderr\n    logging.getLogger().setLevel(original_log_level)\n\nprint(\"üéâ SMA Optimization Complete!\")\nprint(f\"‚ú® Optimal solution: [{sma_result.model[0]:.6f}, {sma_result.model[1]:.6f}]\")\nprint(f\"üéØ Objective value: {sma_result.objective:.8f}\")\nprint(f\"üìä Success: {sma_result.success}\")\n\n# Calculate distance from true optimum\ntrue_optimum = np.array([3.0, 2.0])\ndistance = np.linalg.norm(sma_result.model - true_optimum)\nprint(f\"üìè Distance from true optimum (3, 2): {distance:.6f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure SMA optimization options\nsma_options = InversionOptions()\nsma_options.set_tool(\"mealpy.sma\")\nsma_options.set_params(\n    algorithm=\"OriginalSMA\",  # Use the original SMA variant\n    epoch=100,                # Number of iterations\n    pop_size=50,             # Population size (number of slime moulds)\n    pr=0.03,                 # Probability parameter\n    seed=42,                 # For reproducible results\n    verbose=False            # Reduce output for cleaner notebook\n)\n\nprint(\"‚öôÔ∏è SMA Configuration:\")\nprint(f\"   üîß Algorithm: OriginalSMA\")\nprint(f\"   üîÑ Epochs: 100\")\nprint(f\"   üë• Population: 50 slime moulds\")\nprint(f\"   üé≤ Seed: 42 (reproducible results)\")\n\n# Run the optimization with output suppression\nprint(\"\\nüöÄ Starting SMA optimization...\")\n\n# Suppress mealpy output during optimization\nimport sys\nfrom io import StringIO\nold_stdout = sys.stdout\nsys.stdout = StringIO()\n\ntry:\n    sma_inversion = Inversion(problem, sma_options)\n    sma_result = sma_inversion.run()\nfinally:\n    sys.stdout = old_stdout\n\nprint(\"üéâ SMA Optimization Complete!\")\nprint(f\"‚ú® Optimal solution: [{sma_result.model[0]:.6f}, {sma_result.model[1]:.6f}]\")\nprint(f\"üéØ Objective value: {sma_result.objective:.8f}\")\nprint(f\"üìä Success: {sma_result.success}\")\n\n# Calculate distance from true optimum\ntrue_optimum = np.array([3.0, 2.0])\ndistance = np.linalg.norm(sma_result.model - true_optimum)\nprint(f\"üìè Distance from true optimum (3, 2): {distance:.6f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Compare different SMA algorithms\nalgorithms = [\"OriginalSMA\", \"DevSMA\"]\nresults = {}\ncolors = ['#FF6B6B', '#4ECDC4']\n\nprint(\"üî¨ Comparing SMA Algorithm Variants...\\n\")\n\n# Comprehensive suppression setup\nimport sys\nfrom io import StringIO\n\nfor i, algo in enumerate(algorithms):\n    # Configure options for each algorithm\n    options = InversionOptions()\n    options.set_tool(\"mealpy.sma\")\n    options.set_params(\n        algorithm=algo,\n        epoch=80,\n        pop_size=40,\n        seed=42\n    )\n    \n    # Run optimization with comprehensive suppression\n    print(f\"üß¨ Running {algo}...\")\n    \n    # Store original state\n    original_log_level = logging.getLogger().level\n    old_stdout = sys.stdout\n    old_stderr = sys.stderr\n    \n    # Suppress all output\n    sys.stdout = StringIO()\n    sys.stderr = StringIO()\n    \n    try:\n        suppress_mealpy_logging()\n        inversion = Inversion(problem, options)\n        result = inversion.run()\n    finally:\n        # Restore original state\n        sys.stdout = old_stdout\n        sys.stderr = old_stderr\n        logging.getLogger().setLevel(original_log_level)\n    \n    # Store results\n    results[algo] = {\n        'solution': result.model,\n        'objective': result.objective,\n        'distance': np.linalg.norm(result.model - true_optimum)\n    }\n    \n    print(f\"   ‚úÖ Solution: [{result.model[0]:.4f}, {result.model[1]:.4f}]\")\n    print(f\"   üéØ Objective: {result.objective:.6f}\")\n    print(f\"   üìè Distance: {results[algo]['distance']:.6f}\\n\")\n\nprint(\"üìä Algorithm Comparison Complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare different SMA algorithms\nalgorithms = [\"OriginalSMA\", \"DevSMA\"]\nresults = {}\ncolors = ['#FF6B6B', '#4ECDC4']\n\nprint(\"üî¨ Comparing SMA Algorithm Variants...\\n\")\n\n# Suppress mealpy output during optimization runs\nimport sys\nfrom io import StringIO\n\nfor i, algo in enumerate(algorithms):\n    # Configure options for each algorithm\n    options = InversionOptions()\n    options.set_tool(\"mealpy.sma\")\n    options.set_params(\n        algorithm=algo,\n        epoch=80,\n        pop_size=40,\n        seed=42\n    )\n    \n    # Run optimization with suppressed output\n    print(f\"üß¨ Running {algo}...\")\n    old_stdout = sys.stdout\n    sys.stdout = StringIO()\n    \n    try:\n        inversion = Inversion(problem, options)\n        result = inversion.run()\n    finally:\n        sys.stdout = old_stdout\n    \n    # Store results\n    results[algo] = {\n        'solution': result.model,\n        'objective': result.objective,\n        'distance': np.linalg.norm(result.model - true_optimum)\n    }\n    \n    print(f\"   ‚úÖ Solution: [{result.model[0]:.4f}, {result.model[1]:.4f}]\")\n    print(f\"   üéØ Objective: {result.objective:.6f}\")\n    print(f\"   üìè Distance: {results[algo]['distance']:.6f}\\n\")\n\nprint(\"üìä Algorithm Comparison Complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Solutions on contour plot\n",
    "contour = ax1.contourf(X, Y, Z, levels=levels, cmap='viridis', alpha=0.6)\n",
    "ax1.contour(X, Y, Z, levels=levels, colors='white', alpha=0.3, linewidths=0.5)\n",
    "\n",
    "# Plot solutions\n",
    "for i, (algo, result) in enumerate(results.items()):\n",
    "    ax1.plot(result['solution'][0], result['solution'][1], \n",
    "            'o', color=colors[i], markersize=12, \n",
    "            label=f'{algo}\\n({result[\"solution\"][0]:.3f}, {result[\"solution\"][1]:.3f})',\n",
    "            markeredgecolor='white', markeredgewidth=2)\n",
    "\n",
    "# Mark true optimum\n",
    "ax1.plot(3, 2, '*', color='gold', markersize=20, \n",
    "        label='True Optimum\\n(3.000, 2.000)',\n",
    "        markeredgecolor='black', markeredgewidth=1)\n",
    "\n",
    "ax1.set_xlabel('x‚ÇÅ')\n",
    "ax1.set_ylabel('x‚ÇÇ')\n",
    "ax1.set_title('üó∫Ô∏è Algorithm Solutions on Landscape')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Objective values comparison\n",
    "algos = list(results.keys())\n",
    "objectives = [results[algo]['objective'] for algo in algos]\n",
    "bars = ax2.bar(algos, objectives, color=colors, alpha=0.8, edgecolor='white', linewidth=2)\n",
    "ax2.set_ylabel('Objective Value')\n",
    "ax2.set_title('üéØ Final Objective Values')\n",
    "ax2.set_yscale('log')  # Log scale for better comparison\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, obj in zip(bars, objectives):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{obj:.2e}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 3: Distance from optimum\n",
    "distances = [results[algo]['distance'] for algo in algos]\n",
    "bars = ax3.bar(algos, distances, color=colors, alpha=0.8, edgecolor='white', linewidth=2)\n",
    "ax3.set_ylabel('Distance from True Optimum')\n",
    "ax3.set_title('üìè Accuracy Comparison')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, dist in zip(bars, distances):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{dist:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 4: Performance summary table\n",
    "ax4.axis('tight')\n",
    "ax4.axis('off')\n",
    "\n",
    "# Create table data\n",
    "table_data = []\n",
    "for algo in algos:\n",
    "    result = results[algo]\n",
    "    table_data.append([\n",
    "        algo,\n",
    "        f\"({result['solution'][0]:.4f}, {result['solution'][1]:.4f})\",\n",
    "        f\"{result['objective']:.2e}\",\n",
    "        f\"{result['distance']:.6f}\"\n",
    "    ])\n",
    "\n",
    "table = ax4.table(cellText=table_data,\n",
    "                 colLabels=['Algorithm', 'Solution (x‚ÇÅ, x‚ÇÇ)', 'Objective Value', 'Distance'],\n",
    "                 cellLoc='center',\n",
    "                 loc='center',\n",
    "                 colColours=['lightblue']*4)\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1.2, 2)\n",
    "ax4.set_title('üìã Performance Summary', pad=20, fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Comprehensive algorithm comparison visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Multiple runs for statistical analysis\nn_runs = 10\nseeds = range(42, 42 + n_runs)\n\nprint(f\"üé≤ Running {n_runs} independent SMA optimizations...\\n\")\n\nmulti_results = {\n    'solutions': [],\n    'objectives': [],\n    'distances': []\n}\n\n# Comprehensive suppression setup\nimport sys\nfrom io import StringIO\n\nfor i, seed in enumerate(seeds):\n    # Configure options\n    options = InversionOptions()\n    options.set_tool(\"mealpy.sma\")\n    options.set_params(\n        algorithm=\"OriginalSMA\",\n        epoch=60,\n        pop_size=30,\n        seed=seed\n    )\n    \n    # Store original state\n    original_log_level = logging.getLogger().level\n    old_stdout = sys.stdout\n    old_stderr = sys.stderr\n    \n    # Suppress all output\n    sys.stdout = StringIO()\n    sys.stderr = StringIO()\n    \n    try:\n        suppress_mealpy_logging()\n        inversion = Inversion(problem, options)\n        result = inversion.run()\n    finally:\n        # Restore original state\n        sys.stdout = old_stdout\n        sys.stderr = old_stderr\n        logging.getLogger().setLevel(original_log_level)\n    \n    # Store results\n    multi_results['solutions'].append(result.model)\n    multi_results['objectives'].append(result.objective)\n    distance = np.linalg.norm(result.model - true_optimum)\n    multi_results['distances'].append(distance)\n    \n    print(f\"Run {i+1:2d}: [{result.model[0]:.4f}, {result.model[1]:.4f}] | \"\n          f\"Obj: {result.objective:.2e} | Dist: {distance:.4f}\")\n\n# Statistical analysis\nobjectives = np.array(multi_results['objectives'])\ndistances = np.array(multi_results['distances'])\n\nprint(\"\\nüìà Statistical Summary:\")\nprint(f\"üéØ Objective - Mean: {objectives.mean():.2e}, Std: {objectives.std():.2e}\")\nprint(f\"üìè Distance  - Mean: {distances.mean():.4f}, Std: {distances.std():.4f}\")\nprint(f\"üèÜ Best objective: {objectives.min():.2e}\")\nprint(f\"üéØ Success rate (dist < 0.1): {(distances < 0.1).sum()}/{n_runs} ({100*(distances < 0.1).mean():.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Multiple runs for statistical analysis\nn_runs = 10\nseeds = range(42, 42 + n_runs)\n\nprint(f\"üé≤ Running {n_runs} independent SMA optimizations...\\n\")\n\nmulti_results = {\n    'solutions': [],\n    'objectives': [],\n    'distances': []\n}\n\n# Suppress mealpy output during runs\nimport sys\nfrom io import StringIO\n\nfor i, seed in enumerate(seeds):\n    # Configure options\n    options = InversionOptions()\n    options.set_tool(\"mealpy.sma\")\n    options.set_params(\n        algorithm=\"OriginalSMA\",\n        epoch=60,\n        pop_size=30,\n        seed=seed\n    )\n    \n    # Run optimization with suppressed output\n    old_stdout = sys.stdout\n    sys.stdout = StringIO()\n    \n    try:\n        inversion = Inversion(problem, options)\n        result = inversion.run()\n    finally:\n        sys.stdout = old_stdout\n    \n    # Store results\n    multi_results['solutions'].append(result.model)\n    multi_results['objectives'].append(result.objective)\n    distance = np.linalg.norm(result.model - true_optimum)\n    multi_results['distances'].append(distance)\n    \n    print(f\"Run {i+1:2d}: [{result.model[0]:.4f}, {result.model[1]:.4f}] | \"\n          f\"Obj: {result.objective:.2e} | Dist: {distance:.4f}\")\n\n# Statistical analysis\nobjectives = np.array(multi_results['objectives'])\ndistances = np.array(multi_results['distances'])\n\nprint(\"\\nüìà Statistical Summary:\")\nprint(f\"üéØ Objective - Mean: {objectives.mean():.2e}, Std: {objectives.std():.2e}\")\nprint(f\"üìè Distance  - Mean: {distances.mean():.4f}, Std: {distances.std():.4f}\")\nprint(f\"üèÜ Best objective: {objectives.min():.2e}\")\nprint(f\"üéØ Success rate (dist < 0.1): {(distances < 0.1).sum()}/{n_runs} ({100*(distances < 0.1).mean():.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize statistical performance\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: All solutions on landscape\n",
    "contour = ax1.contourf(X, Y, Z, levels=levels, cmap='viridis', alpha=0.6)\n",
    "ax1.contour(X, Y, Z, levels=levels, colors='white', alpha=0.3, linewidths=0.5)\n",
    "\n",
    "# Plot all solutions\n",
    "solutions = np.array(multi_results['solutions'])\n",
    "scatter = ax1.scatter(solutions[:, 0], solutions[:, 1], \n",
    "                     c=multi_results['objectives'], \n",
    "                     cmap='Reds', s=100, alpha=0.8, \n",
    "                     edgecolors='white', linewidth=2)\n",
    "\n",
    "# Mark true optimum\n",
    "ax1.plot(3, 2, '*', color='gold', markersize=20, \n",
    "        markeredgecolor='black', markeredgewidth=1, label='True Optimum')\n",
    "\n",
    "ax1.set_xlabel('x‚ÇÅ')\n",
    "ax1.set_ylabel('x‚ÇÇ')\n",
    "ax1.set_title(f'üó∫Ô∏è All {n_runs} Solutions on Landscape')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax1, label='Objective Value')\n",
    "\n",
    "# Plot 2: Objective value distribution\n",
    "ax2.hist(objectives, bins=8, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax2.axvline(objectives.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {objectives.mean():.2e}')\n",
    "ax2.set_xlabel('Objective Value')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('üìä Objective Value Distribution')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Distance distribution\n",
    "ax3.hist(distances, bins=8, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "ax3.axvline(distances.mean(), color='blue', linestyle='--', linewidth=2, label=f'Mean: {distances.mean():.4f}')\n",
    "ax3.axvline(0.1, color='green', linestyle=':', linewidth=2, label='Success Threshold (0.1)')\n",
    "ax3.set_xlabel('Distance from True Optimum')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('üìè Distance Distribution')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Convergence trend\n",
    "run_numbers = range(1, n_runs + 1)\n",
    "ax4.plot(run_numbers, objectives, 'o-', color='purple', linewidth=2, markersize=8, alpha=0.7)\n",
    "ax4.axhline(objectives.mean(), color='red', linestyle='--', alpha=0.7, label=f'Mean: {objectives.mean():.2e}')\n",
    "ax4.fill_between(run_numbers, \n",
    "                objectives.mean() - objectives.std(), \n",
    "                objectives.mean() + objectives.std(), \n",
    "                alpha=0.2, color='purple', label='¬±1 Std Dev')\n",
    "ax4.set_xlabel('Run Number')\n",
    "ax4.set_ylabel('Objective Value')\n",
    "ax4.set_title('üîÑ Performance Across Runs')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Statistical analysis visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Demonstrate CoFI's \"define once, solve many ways\" principle\noptimizers = [\n    {\n        'name': 'SMA (OriginalSMA)',\n        'tool': 'mealpy.sma',\n        'params': {'algorithm': 'OriginalSMA', 'epoch': 50, 'pop_size': 30, 'seed': 42},\n        'color': '#FF6B6B'\n    },\n    {\n        'name': 'SMA (DevSMA)',\n        'tool': 'mealpy.slime_mould',  # Test the alias!\n        'params': {'algorithm': 'DevSMA', 'epoch': 50, 'pop_size': 30, 'seed': 42},\n        'color': '#4ECDC4'\n    },\n    {\n        'name': 'Border Collie',\n        'tool': 'cofi.border_collie_optimization',\n        'params': {'number_of_iterations': 50, 'seed': 42},\n        'color': '#45B7D1'\n    }\n]\n\nprint(\"üîó CoFI Framework Demonstration: Same Problem, Multiple Solvers\\n\")\nprint(\"üìù Problem Definition (defined once):\")\nprint(f\"   üéØ Objective: Modified Himmelblau function\")\nprint(f\"   üìè Dimensions: 2D\")\nprint(f\"   üî≤ Bounds: [-6, 6] √ó [-6, 6]\")\nprint(\"\\nüöÄ Testing Multiple Optimizers...\\n\")\n\noptimizer_results = {}\n\n# Comprehensive suppression setup\nimport sys\nfrom io import StringIO\n\nfor opt in optimizers:\n    try:\n        # Same problem, different solver\n        options = InversionOptions()\n        options.set_tool(opt['tool'])\n        options.set_params(**opt['params'])\n        \n        print(f\"üß¨ Running {opt['name']}...\")\n        \n        # Apply comprehensive suppression for mealpy tools only\n        if 'mealpy' in opt['tool']:\n            # Store original state\n            original_log_level = logging.getLogger().level\n            old_stdout = sys.stdout\n            old_stderr = sys.stderr\n            \n            # Suppress all output\n            sys.stdout = StringIO()\n            sys.stderr = StringIO()\n            \n            try:\n                suppress_mealpy_logging()\n                inversion = Inversion(problem, options)  # Same problem definition!\n                result = inversion.run()\n            finally:\n                # Restore original state\n                sys.stdout = old_stdout\n                sys.stderr = old_stderr\n                logging.getLogger().setLevel(original_log_level)\n        else:\n            # Run normally for non-mealpy tools\n            inversion = Inversion(problem, options)\n            result = inversion.run()\n        \n        distance = np.linalg.norm(result.model - true_optimum)\n        optimizer_results[opt['name']] = {\n            'solution': result.model,\n            'objective': result.objective,\n            'distance': distance,\n            'color': opt['color'],\n            'success': result.success\n        }\n        \n        print(f\"   ‚úÖ Solution: [{result.model[0]:.4f}, {result.model[1]:.4f}]\")\n        print(f\"   üéØ Objective: {result.objective:.6f}\")\n        print(f\"   üìè Distance: {distance:.6f}\")\n        print(f\"   ‚úîÔ∏è Success: {result.success}\\n\")\n        \n    except Exception as e:\n        print(f\"   ‚ùå Failed: {str(e)[:50]}...\\n\")\n        optimizer_results[opt['name']] = None\n\nprint(\"üéâ Multi-optimizer comparison complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate CoFI's \"define once, solve many ways\" principle\noptimizers = [\n    {\n        'name': 'SMA (OriginalSMA)',\n        'tool': 'mealpy.sma',\n        'params': {'algorithm': 'OriginalSMA', 'epoch': 50, 'pop_size': 30, 'seed': 42},\n        'color': '#FF6B6B'\n    },\n    {\n        'name': 'SMA (DevSMA)',\n        'tool': 'mealpy.slime_mould',  # Test the alias!\n        'params': {'algorithm': 'DevSMA', 'epoch': 50, 'pop_size': 30, 'seed': 42},\n        'color': '#4ECDC4'\n    },\n    {\n        'name': 'Border Collie',\n        'tool': 'cofi.border_collie_optimization',\n        'params': {'number_of_iterations': 50, 'seed': 42},\n        'color': '#45B7D1'\n    }\n]\n\nprint(\"üîó CoFI Framework Demonstration: Same Problem, Multiple Solvers\\n\")\nprint(\"üìù Problem Definition (defined once):\")\nprint(f\"   üéØ Objective: Modified Himmelblau function\")\nprint(f\"   üìè Dimensions: 2D\")\nprint(f\"   üî≤ Bounds: [-6, 6] √ó [-6, 6]\")\nprint(\"\\nüöÄ Testing Multiple Optimizers...\\n\")\n\noptimizer_results = {}\n\n# Suppress mealpy output during runs\nimport sys\nfrom io import StringIO\n\nfor opt in optimizers:\n    try:\n        # Same problem, different solver\n        options = InversionOptions()\n        options.set_tool(opt['tool'])\n        options.set_params(**opt['params'])\n        \n        print(f\"üß¨ Running {opt['name']}...\")\n        \n        # Suppress output for SMA runs only\n        if 'mealpy' in opt['tool']:\n            old_stdout = sys.stdout\n            sys.stdout = StringIO()\n        \n        try:\n            inversion = Inversion(problem, options)  # Same problem definition!\n            result = inversion.run()\n        finally:\n            if 'mealpy' in opt['tool']:\n                sys.stdout = old_stdout\n        \n        distance = np.linalg.norm(result.model - true_optimum)\n        optimizer_results[opt['name']] = {\n            'solution': result.model,\n            'objective': result.objective,\n            'distance': distance,\n            'color': opt['color'],\n            'success': result.success\n        }\n        \n        print(f\"   ‚úÖ Solution: [{result.model[0]:.4f}, {result.model[1]:.4f}]\")\n        print(f\"   üéØ Objective: {result.objective:.6f}\")\n        print(f\"   üìè Distance: {distance:.6f}\")\n        print(f\"   ‚úîÔ∏è Success: {result.success}\\n\")\n        \n    except Exception as e:\n        print(f\"   ‚ùå Failed: {str(e)[:50]}...\\n\")\n        optimizer_results[opt['name']] = None\n\nprint(\"üéâ Multi-optimizer comparison complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimizer comparison\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# Plot 1: All optimizer solutions on landscape\n",
    "contour = ax1.contourf(X, Y, Z, levels=levels, cmap='viridis', alpha=0.5)\n",
    "ax1.contour(X, Y, Z, levels=levels, colors='white', alpha=0.3, linewidths=0.5)\n",
    "\n",
    "# Plot solutions from each optimizer\n",
    "valid_results = {name: result for name, result in optimizer_results.items() if result is not None}\n",
    "\n",
    "for name, result in valid_results.items():\n",
    "    ax1.plot(result['solution'][0], result['solution'][1], \n",
    "            'o', color=result['color'], markersize=12, \n",
    "            label=f\"{name}\\n({result['solution'][0]:.3f}, {result['solution'][1]:.3f})\",\n",
    "            markeredgecolor='white', markeredgewidth=2)\n",
    "\n",
    "# Mark true optimum\n",
    "ax1.plot(3, 2, '*', color='gold', markersize=20, \n",
    "        label='True Optimum\\n(3.000, 2.000)',\n",
    "        markeredgecolor='black', markeredgewidth=1)\n",
    "\n",
    "ax1.set_xlabel('x‚ÇÅ')\n",
    "ax1.set_ylabel('x‚ÇÇ')\n",
    "ax1.set_title('üó∫Ô∏è Multi-Optimizer Solutions')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Objective values comparison\n",
    "names = list(valid_results.keys())\n",
    "objectives = [valid_results[name]['objective'] for name in names]\n",
    "colors = [valid_results[name]['color'] for name in names]\n",
    "\n",
    "bars = ax2.bar(range(len(names)), objectives, color=colors, alpha=0.8, \n",
    "              edgecolor='white', linewidth=2)\n",
    "ax2.set_xticks(range(len(names)))\n",
    "ax2.set_xticklabels([name.split('(')[0].strip() for name in names], rotation=45, ha='right')\n",
    "ax2.set_ylabel('Objective Value (log scale)')\n",
    "ax2.set_title('üéØ Objective Values Comparison')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, obj in zip(bars, objectives):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{obj:.2e}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Plot 3: Distance comparison\n",
    "distances = [valid_results[name]['distance'] for name in names]\n",
    "bars = ax3.bar(range(len(names)), distances, color=colors, alpha=0.8, \n",
    "              edgecolor='white', linewidth=2)\n",
    "ax3.set_xticks(range(len(names)))\n",
    "ax3.set_xticklabels([name.split('(')[0].strip() for name in names], rotation=45, ha='right')\n",
    "ax3.set_ylabel('Distance from True Optimum')\n",
    "ax3.set_title('üìè Accuracy Comparison')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, dist in zip(bars, distances):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{dist:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Plot 4: Performance radar chart\n",
    "ax4.axis('off')\n",
    "\n",
    "# Create performance summary table\n",
    "table_data = []\n",
    "headers = ['Optimizer', 'Solution (x‚ÇÅ, x‚ÇÇ)', 'Objective', 'Distance', 'Rank']\n",
    "\n",
    "# Sort by objective value for ranking\n",
    "sorted_results = sorted(valid_results.items(), key=lambda x: x[1]['objective'])\n",
    "\n",
    "for rank, (name, result) in enumerate(sorted_results, 1):\n",
    "    table_data.append([\n",
    "        name.split('(')[0].strip(),\n",
    "        f\"({result['solution'][0]:.4f}, {result['solution'][1]:.4f})\",\n",
    "        f\"{result['objective']:.2e}\",\n",
    "        f\"{result['distance']:.6f}\",\n",
    "        f\"#{rank}\"\n",
    "    ])\n",
    "\n",
    "table = ax4.table(cellText=table_data,\n",
    "                 colLabels=headers,\n",
    "                 cellLoc='center',\n",
    "                 loc='center',\n",
    "                 colColours=['lightblue']*5)\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.3, 2.5)\n",
    "ax4.set_title('üèÜ Performance Ranking', pad=20, fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéâ CoFI's unified framework demonstration complete!\")\n",
    "print(\"‚ú® One problem definition, multiple solvers - that's the power of CoFI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Insights and Conclusions\n",
    "\n",
    "### üß¨ Slime Mould Algorithm Performance\n",
    "\n",
    "From our experiments, we can observe several key characteristics of the SMA:\n",
    "\n",
    "1. **üéØ Excellent Global Optimization**: SMA consistently finds solutions very close to the global optimum\n",
    "2. **üîÑ Robust Convergence**: Multiple runs show consistent performance with low variance\n",
    "3. **‚ö° Efficient Exploration**: The algorithm effectively explores the multi-modal landscape\n",
    "4. **üß† Bio-inspired Intelligence**: The slime mould foraging behavior translates well to optimization\n",
    "\n",
    "### üîó CoFI Integration Benefits\n",
    "\n",
    "The integration of SMA into CoFI demonstrates several advantages:\n",
    "\n",
    "- **üéØ Unified Interface**: Same problem definition works with all optimizers\n",
    "- **üîß Easy Configuration**: Simple parameter setting and algorithm selection\n",
    "- **üìä Consistent Results**: Standardized result format across all tools\n",
    "- **üöÄ Extensibility**: Easy to add new algorithms and compare performance\n",
    "\n",
    "### üåç Applications in Geophysics\n",
    "\n",
    "SMA is particularly well-suited for geophysical inverse problems because:\n",
    "\n",
    "- **üóª Multi-modal Landscapes**: Many geophysical problems have multiple local minima\n",
    "- **üö´ Gradient-free**: Works with non-differentiable objective functions\n",
    "- **üé≤ Global Search**: Excellent for finding global solutions in complex parameter spaces\n",
    "- **üí™ Robustness**: Handles noisy and discontinuous objective functions well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps and Further Exploration\n",
    "\n",
    "### üî¨ Advanced Usage\n",
    "\n",
    "```python\n",
    "# Example: Using SMA with spatial regularization\n",
    "from cofi.utils import QuadraticReg\n",
    "\n",
    "# Define a 2D tomography problem with spatial smoothing\n",
    "model_shape = (20, 15)  # 2D grid\n",
    "spatial_reg = QuadraticReg(model_shape=model_shape, weighting_matrix=\"smoothing\")\n",
    "\n",
    "def tomography_objective(slowness):\n",
    "    data_misfit = compute_travel_time_misfit(slowness)  # Your forward model\n",
    "    regularization = spatial_reg(slowness)             # Spatial smoothing\n",
    "    return data_misfit + 0.1 * regularization          # Combined objective\n",
    "\n",
    "# CoFI handles the vector abstraction automatically!\n",
    "problem = BaseProblem()\n",
    "problem.set_objective(tomography_objective)\n",
    "problem.set_model_shape(model_shape)  # 2D spatial model\n",
    "problem.set_bounds((1.0, 5.0))        # Slowness bounds\n",
    "\n",
    "# SMA works seamlessly with spatial regularization\n",
    "options = InversionOptions()\n",
    "options.set_tool(\"mealpy.sma\")\n",
    "options.set_params(epoch=200, pop_size=100)\n",
    "\n",
    "result = Inversion(problem, options).run()\n",
    "```\n",
    "\n",
    "### üìö Further Reading\n",
    "\n",
    "- **Original SMA Paper**: Li et al. (2020) - Slime mould algorithm: A new method for stochastic optimization\n",
    "- **CoFI Documentation**: [cofi.readthedocs.io](https://cofi.readthedocs.io)\n",
    "- **Mealpy Library**: [mealpy.readthedocs.io](https://mealpy.readthedocs.io)\n",
    "- **Bio-inspired Optimization**: Yang (2020) - Nature-Inspired Optimization Algorithms\n",
    "\n",
    "### üõ†Ô∏è Tool Parameters\n",
    "\n",
    "Key SMA parameters to experiment with:\n",
    "\n",
    "- **`epoch`**: Number of iterations (higher = more thorough search)\n",
    "- **`pop_size`**: Population size (higher = better exploration)\n",
    "- **`pr`**: Probability parameter (controls exploration/exploitation balance)\n",
    "- **`algorithm`**: Choose between \"OriginalSMA\" and \"DevSMA\"\n",
    "- **`mode`**: Parallel execution (\"thread\", \"process\", \"swarm\")\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Thank You!\n",
    "\n",
    "**You've successfully explored the Slime Mould Algorithm integration in CoFI!**\n",
    "\n",
    "This notebook demonstrated how bio-inspired optimization can be seamlessly integrated into geophysical inverse problems using CoFI's elegant framework. The SMA provides a powerful tool for global optimization, particularly effective for complex, multi-modal landscapes commonly encountered in geophysics.\n",
    "\n",
    "Happy optimizing! üß¨üåç‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final celebration plot! üéâ\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "# Create a beautiful final visualization\n",
    "contour = ax.contourf(X, Y, Z, levels=levels, cmap='plasma', alpha=0.8)\n",
    "ax.contour(X, Y, Z, levels=levels, colors='white', alpha=0.4, linewidths=0.5)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "cbar.set_label('Function Value', fontsize=14)\n",
    "\n",
    "# Mark the true optimum with a beautiful star\n",
    "ax.plot(3, 2, '*', color='gold', markersize=30, \n",
    "        markeredgecolor='black', markeredgewidth=2, \n",
    "        label='Global Optimum Found by SMA! ‚≠ê')\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_xlabel('x‚ÇÅ', fontsize=14)\n",
    "ax.set_ylabel('x‚ÇÇ', fontsize=14)\n",
    "ax.set_title('üß¨ Slime Mould Algorithm Successfully Integrated in CoFI! üéâ\\n'\n",
    "            'Bio-inspired Optimization for Geophysical Inverse Problems', \n",
    "            fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=14, loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add celebration text\n",
    "ax.text(0.02, 0.98, '‚ú® Mission Accomplished! ‚ú®', \n",
    "        transform=ax.transAxes, fontsize=12, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8),\n",
    "        verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéä Congratulations! üéä\")\n",
    "print(\"üß¨ You've successfully explored the Slime Mould Algorithm in CoFI!\")\n",
    "print(\"üåü Bio-inspired optimization is now at your fingertips!\")\n",
    "print(\"üöÄ Ready to tackle real geophysical inverse problems!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  Thank you for exploring CoFI's SMA integration! üôè\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}